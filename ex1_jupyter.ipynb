{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1916498b",
   "metadata": {},
   "source": [
    "# 67658 Natural Language Processing\n",
    "## &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Exercise 1\n",
    "### &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Asaf Shul 207042714\n",
    "### &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Daniel Azulay 311119895"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be67d2b",
   "metadata": {},
   "source": [
    "initialize notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64b4ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "\n",
    "# uncomment to download the module:\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71ac9d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/asafshul/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    }
   ],
   "source": [
    "# load the data:\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
    "corpus = dataset['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d279c",
   "metadata": {},
   "source": [
    "#### functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61eaea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return 'START ' + ' '.join([c.lemma_ for c in nlp(text) if c.is_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17144986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_language_model(corpus, level=2):\n",
    "    ngrams = []\n",
    "    print('counting words...')\n",
    "    \n",
    "    # extract_ngrams:\n",
    "    for text in corpus:\n",
    "        ngrams.extend(list(textacy.extract.ngrams(nlp(clean_text(text)), level, filter_stops=False)))\n",
    "    \n",
    "    print('calculating freqs...')\n",
    "    if level==1:\n",
    "        return get_unigram(ngrams)\n",
    "    if level==2:\n",
    "        return get_bigram(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adaff730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigram(ngrams):\n",
    "    word_counts = dict(Counter([str(w) for w in ngrams]))\n",
    "    _N = sum(word_counts.values())\n",
    "    \n",
    "    for key in word_counts:\n",
    "        word_counts[key] = np.log(word_counts[key] / _N)\n",
    "\n",
    "    return word_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a97dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram(ngrams):\n",
    "    base_dict = dict(Counter([(str(w[0]), str(w[1])) for w in ngrams]))\n",
    "    freq_dict = {}\n",
    "    \n",
    "    # format to bigram dict:\n",
    "    for key, count in base_dict.items():\n",
    "        base_word, next_word = key\n",
    "        if base_word not in freq_dict:\n",
    "            freq_dict[base_word] = {next_word : count}\n",
    "        elif next_word not in freq_dict[base_word]:\n",
    "            freq_dict[base_word][next_word] = count\n",
    "        else: \n",
    "            freq_dict[base_word][next_word] += count\n",
    "    \n",
    "    # change count to relative probability:\n",
    "    for word_counts in freq_dict.values():\n",
    "        _N = sum(word_counts.values())\n",
    "        for key in word_counts:\n",
    "            word_counts[key] = np.log(word_counts[key] / _N)\n",
    "    \n",
    "    # return the model:\n",
    "    return freq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b7604d",
   "metadata": {},
   "source": [
    "**1.** \n",
    "\n",
    "Train maximum-likelihood unigram and bigram language models based on the above training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb242199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting words...\n",
      "calculating freqs...\n",
      "CPU times: user 10min 57s, sys: 2.48 s, total: 11min\n",
      "Wall time: 11min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# unigram:\n",
    "uni_model = train_language_model(corpus, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79b18f2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting words...\n",
      "calculating freqs...\n",
      "CPU times: user 10min 54s, sys: 4.89 s, total: 10min 59s\n",
      "Wall time: 11min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# bigram:\n",
    "bi_model = train_language_model(corpus, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5016212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models to pickle file:\n",
    "\n",
    "# with open('unigram.pickle', 'wb') as file:\n",
    "#     pickle.dump(uni_model, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('bigram.pickle', 'wb') as file:\n",
    "#     pickle.dump(bi_model, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b4ba03",
   "metadata": {},
   "source": [
    "2. Using the bigram model, continue the following sentence with the most probable word predicted by the model: “ I have a house in ...”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c32044d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_bigram(model, sentance):\n",
    "    last_word = sentance.split(' ')[-1]\n",
    "    probs = bi_model[last_word]\n",
    "    idx = np.argmax(list(probs.values()))\n",
    "    return list(probs.keys())[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54be42b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentance = 'I have a house in'\n",
    "pred_bigram(bi_model, sentance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610073b8",
   "metadata": {},
   "source": [
    "3. Using the bigram model:\n",
    "\n",
    "(a) compute the probability of the following two sentences (for each sentence separately).\n",
    "\n",
    "(b) compute the perplexity of both the following two sentences (treating them as a single test set with 2 sentences).\n",
    "\n",
    "- Brad Pitt was born in Oklahoma \n",
    "- The actor was born in USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4fed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sentance_log_prob_bigram(sentance, bi_model):\n",
    "    sentance_arr = clean_text(sentance).split(' ')\n",
    "    prob = 0\n",
    "    \n",
    "    for word, next_word in zip(sentance_arr[:-1], sentance_arr[1:]):\n",
    "        if word in bi_model and next_word in bi_model[word]:\n",
    "            prob += bi_model[word][next_word]\n",
    "        else:\n",
    "            return -np.inf\n",
    "    \n",
    "    return round(prob, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37863be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- prob for \"Brad Pitt was born in Oklahoma\" is: -inf\n",
      "- prob for \"The actor was born in USA\" is: -29.687\n"
     ]
    }
   ],
   "source": [
    "sentance1 = 'Brad Pitt was born in Oklahoma'\n",
    "sentance2 = 'The actor was born in USA'\n",
    "\n",
    "print(f'- prob for \"{sentance1}\" is: {calc_sentance_log_prob_bigram(sentance1, bi_model)}')\n",
    "print(f'- prob for \"{sentance2}\" is: {calc_sentance_log_prob_bigram(sentance2, bi_model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5e323",
   "metadata": {},
   "source": [
    "4. \n",
    "Now we use linear interpolation smoothing between the bigram model and unigram model with:\n",
    "- λbigram = 2/3 \n",
    "- λ_unigram = 1/3\n",
    "\n",
    "using the same training data. \n",
    "\n",
    "Given this new model, compute the probability and the perplexity of the same sentences such as in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1026532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sentance_log_prob_interpulation(sentance, uni_model, bi_model, lam_bi, lam_uni):\n",
    "    sentance_arr = clean_text(sentance).split(' ')\n",
    "    prob = 1\n",
    "    \n",
    "    for word, next_word in zip(sentance_arr[:-1], sentance_arr[1:]):\n",
    "        if (word in bi_model and next_word in bi_model[word]) and (next_word in uni_model):\n",
    "            prob *= (lam_bi * np.exp(bi_model[word][next_word])) + (lam_uni * np.exp(uni_model[next_word]))\n",
    "        \n",
    "        elif word in bi_model and next_word in bi_model[word]:\n",
    "            prob *= (lam_bi * np.exp(bi_model[word][next_word]))\n",
    "        \n",
    "        elif next_word in uni_model:\n",
    "            prob *= (lam_uni * np.exp(uni_model[next_word]))\n",
    "        \n",
    "        else:\n",
    "            return -np.inf\n",
    "    \n",
    "    return np.log(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bce8347",
   "metadata": {},
   "outputs": [],
   "source": [
    "lam_bi = 2/3\n",
    "lam_uni = 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64c8bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- prob for \"Brad Pitt was born in Oklahoma\" is: -36.20826629122097\n",
      "- prob for \"The actor was born in USA\" is: -31.017958929308932\n"
     ]
    }
   ],
   "source": [
    "# probability:\n",
    "sents1_prob = calc_sentance_log_prob_interpulation(sentance1, uni_model, bi_model, lam_bi, lam_uni)\n",
    "sents2_prob = calc_sentance_log_prob_interpulation(sentance2, uni_model, bi_model, lam_bi, lam_uni)\n",
    "print(f'- prob for \"{sentance1}\" is: {sents1_prob}')\n",
    "print(f'- prob for \"{sentance2}\" is: {sents2_prob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fe7fe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity in test set is is: 271.0180530398554\n"
     ]
    }
   ],
   "source": [
    "# perplexity:\n",
    "M = len(clean_text(sentance1).split(' ') + clean_text(sentance2).split(' ')) - 2\n",
    "print(f'perplexity in test set is is: {np.exp(-((sents1_prob + sents2_prob) / M))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85c027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
